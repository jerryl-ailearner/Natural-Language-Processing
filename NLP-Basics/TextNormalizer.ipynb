{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    soup=BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe','script'])]\n",
    "    stripped_text=soup.get_text()\n",
    "    stripped_text=re.sub(r'[\\r|\\n|\\r\\n]+','\\n',stripped_text)\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text=unicodedata.normalize('NFKD', text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern=re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match=contraction.group(0)\n",
    "        first_char=match[0]\n",
    "        expanded_contraction=contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction=first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text=contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text=re.sub(\"'\",\"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text=nlp(text)\n",
    "    text=' '.join([word.lemma_ if word.lemma_!='-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern=r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens=tokenizer.tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens=[token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens=[token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True, accented_char_removal=True, text_lower_case=True, text_lemmatization=True, special_char_removal=True, stopword_removal=True, remove_digits=True):\n",
    "    normalized_corpus=[]\n",
    "    for doc in corpus:\n",
    "        if html_stripping:\n",
    "            doc=strip_html_tags(doc)\n",
    "        if accented_char_removal:\n",
    "            doc=remove_accented_chars(doc)\n",
    "        if contraction_expansion:\n",
    "            doc=expand_contractions(doc)\n",
    "        if text_lower_case:\n",
    "            doc=doc.lower()\n",
    "        doc=re.sub(r'[\\r|\\n|\\r\\n]+',' ',doc)\n",
    "        if text_lemmatization:\n",
    "            doc=lemmatize_text(doc)\n",
    "        if special_char_removal:\n",
    "            special_char_pattern=re.compile(r'([{.(-)}])')\n",
    "            doc=special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc=remove_special_characters(doc, remove_digits=remove_digits)\n",
    "        doc=re.sub(' +',' ',doc)\n",
    "        if stopword_removal:\n",
    "            doc=remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "        \n",
    "        normalized_corpus.append(doc)\n",
    "    return normalized_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=(\"US unveils world's most powerful supercomputer, bears China. The US has unveiled the world's most powerful supercomputer called 'Summit',\"\"beating the previous record-holder China's Sunway TaihuLight. With a peak performance\"\"of 200,000 trillion calculations per second, it is over twice as fast as Sunway Taihulight, \"\"which is capable of 93000 trillion calculations per second. Summit has 4608 servers, \"\"which reportedly take up the size of two tennis courts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us unveil world powerful supercomputer bear china us unveil world powerful supercomputer call summit beat previous record holder chinas sunway taihulight peak performanceof trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_corpus([sample_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
